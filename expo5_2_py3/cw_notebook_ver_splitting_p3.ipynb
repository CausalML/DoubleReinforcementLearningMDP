{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L1RBkFLR0Y_Y",
        "outputId": "aa21aeea-540c-4ea5-d56a-3c019d7824c1"
      },
      "id": "L1RBkFLR0Y_Y",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-12T20:03:14.713896Z",
          "start_time": "2025-05-12T20:03:05.105703Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50f1f594d24d639c",
        "outputId": "2389fc60-833f-4595-9b8b-3a910445f19f"
      },
      "cell_type": "code",
      "source": [
        "pip install gym matplotlib numpy pandas scipy"
      ],
      "id": "50f1f594d24d639c",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.11/dist-packages (0.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gym) (3.1.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.11/dist-packages (from gym) (0.0.8)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import sys\n",
        "import gym\n",
        "\n",
        "# Since lib.envs isn't available, we'll need to define these environments here\n",
        "# or use gym environments directly. For now, I'll create simplified versions.\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Simple CliffWalkingEnv implementation\n",
        "class CliffWalkingEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.shape = (4, 12)\n",
        "        self.start_state_index = np.ravel_multi_index((3, 0), self.shape)\n",
        "        self.goal_state_index = np.ravel_multi_index((3, 11), self.shape)\n",
        "        self.cliff = list(range(np.ravel_multi_index((3, 1), self.shape),\n",
        "                               np.ravel_multi_index((3, 11), self.shape)))\n",
        "        self.nS = self.shape[0] * self.shape[1]\n",
        "        self.nA = 4  # up, right, down, left\n",
        "\n",
        "        # Calculate transition probabilities and rewards\n",
        "        self.P = {}\n",
        "        for s in range(self.nS):\n",
        "            position = np.unravel_index(s, self.shape)\n",
        "            self.P[s] = {a: [] for a in range(self.nA)}\n",
        "\n",
        "            # Actions: 0=up, 1=right, 2=down, 3=left\n",
        "            for a in range(self.nA):\n",
        "                reward = -1.0  # default reward for each move\n",
        "                next_position = list(position)\n",
        "                if a == 0:\n",
        "                    next_position[0] = max(position[0] - 1, 0)\n",
        "                elif a == 1:\n",
        "                    next_position[1] = min(position[1] + 1, self.shape[1] - 1)\n",
        "                elif a == 2:\n",
        "                    next_position[0] = min(position[0] + 1, self.shape[0] - 1)\n",
        "                elif a == 3:\n",
        "                    next_position[1] = max(position[1] - 1, 0)\n",
        "\n",
        "                next_state = np.ravel_multi_index(next_position, self.shape)\n",
        "\n",
        "                # Check if we're at the cliff\n",
        "                if s in self.cliff:\n",
        "                    next_state = self.start_state_index\n",
        "                    reward = -100.0\n",
        "\n",
        "                # Check if we're at the goal\n",
        "                done = next_state == self.goal_state_index\n",
        "\n",
        "                self.P[s][a] = [(1.0, next_state, reward, done)]\n",
        "\n",
        "        self.observation_space = gym.spaces.Discrete(self.nS)\n",
        "        self.action_space = gym.spaces.Discrete(self.nA)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done, _ = self._step(action)\n",
        "        self.s = state\n",
        "        return (state, reward, done, {})\n",
        "\n",
        "    def _step(self, action):\n",
        "        (probs, next_state, reward, done) = self.P[self.s][action][0]\n",
        "        return (next_state, reward, done, {})\n",
        "\n",
        "    def reset(self):\n",
        "        self.s = self.start_state_index\n",
        "        return self.s\n",
        "\n",
        "# Simple WindyGridworldEnv implementation (not fully used in this code)\n",
        "class WindyGridworldEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.shape = (7, 10)\n",
        "        self.nS = self.shape[0] * self.shape[1]\n",
        "        self.nA = 4  # up, right, down, left\n",
        "        self.wind = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        # Not implemented as it's not used in the main code\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        self.s = np.ravel_multi_index((3, 0), self.shape)\n",
        "        return self.s\n",
        "\n",
        "from scipy.optimize import minimize, rosen, rosen_der\n",
        "from scipy.optimize import Bounds\n",
        "\n",
        "bounds = Bounds([-0.1, -0.1], [0.1, 0.1])\n",
        "\n",
        "env = CliffWalkingEnv()\n",
        "\n",
        "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
        "    def policy_fn(observation):\n",
        "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
        "        best_action = np.argmax(Q[observation])\n",
        "        A[best_action] += (1.0 - epsilon)\n",
        "        return A\n",
        "    return policy_fn\n",
        "\n",
        "# Update these paths to your actual file locations\n",
        "Q_space = np.load(\"/content/drive/MyDrive/DoubleReinforcementLearningMDP-master/Q-table-cliff.npz\")[\"xxx\"]\n",
        "Q_space2 = np.load(\"/content/drive/MyDrive/DoubleReinforcementLearningMDP-master/Q-table-cliff.npz\")[\"xxx\"]\n",
        "\n",
        "prob1 = [1.0 for i in range((env.nA))]\n",
        "prob1 = prob1/np.sum(prob1)\n",
        "\n",
        "betabeta = 0.8\n",
        "def sample_policy(observation, alpha=0.9):\n",
        "    prob2 = alpha*Q_space[observation,:] + (1-alpha)*prob1\n",
        "    return np.random.choice(env.nA, 1, p=prob2)[0]\n",
        "\n",
        "\n",
        "def behavior_policy(observation, beta=betabeta):\n",
        "    prob2 = beta*Q_space[observation,:] + (1-beta)*prob1\n",
        "    return np.random.choice(env.nA, 1, p=prob2)[0]\n",
        "\n",
        "\n",
        "def target_dense(observation, alpha=0.9):\n",
        "    prob2 = alpha*Q_space[observation,:] + (1-alpha)*prob1\n",
        "    return prob2\n",
        "\n",
        "def behav_dense(observation, beta=betabeta):\n",
        "    prob2 = beta*Q_space[observation,:] + (1-beta)*prob1\n",
        "    return prob2\n",
        "\n",
        "# FIXED: Matching the original notebook exactly\n",
        "def sarsa2(env, policy, policy2, num_episodes, discount_factor=1.0, Q_space2=Q_space2, alpha=0.6, epsilon=0.03):\n",
        "    \"\"\"\n",
        "    Expected SARSA implementation matching the original notebook\n",
        "    \"\"\"\n",
        "    # Initialize Q as a copy of Q_space2 (not zeros)\n",
        "    Q = np.copy(Q_space2)\n",
        "    episode_episode = []\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        if (i_episode + 1) % 200 == 0:\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        state = env.reset()\n",
        "        action = policy2(state)\n",
        "        episode = []\n",
        "\n",
        "        for t in itertools.count():\n",
        "            # Take a step\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "\n",
        "            # Pick the next action\n",
        "            next_action = policy2(next_state)\n",
        "\n",
        "            # TD Update - Expected SARSA without importance sampling\n",
        "            td_target = reward + discount_factor * np.sum(Q[next_state,:]*target_dense(next_state))\n",
        "            td_delta = td_target - Q[state, action]\n",
        "            Q[state, action] += alpha * td_delta  # No importance sampling correction\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            action = next_action\n",
        "            state = next_state\n",
        "\n",
        "        episode_episode.append(episode)\n",
        "\n",
        "    # Return only Q and episode_episode (matching original)\n",
        "    return Q, episode_episode\n",
        "\n",
        "bounds = Bounds([-0.2, -0.2], [0.2, 0.2])\n",
        "def sigmoid(x, derivative=False):\n",
        "    return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
        "\n",
        "depth = 1\n",
        "def mc_prediction(env, policy, policy2, episode_episode, Q_=1.0, num_episodes=100, discount_factor=1.0):\n",
        "    \"\"\"\n",
        "    Monte Carlo prediction for policy evaluation\n",
        "    \"\"\"\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "    returns_count2 = defaultdict(float)\n",
        "\n",
        "    predic_list = []\n",
        "    predic_list2 = []\n",
        "    predic_list3 = []\n",
        "    predic_list22 = []\n",
        "    predic_list4 = []\n",
        "    predic_list5 = np.ones(num_episodes)\n",
        "    auxiauxi = []\n",
        "    epiepi = []\n",
        "    weight_list = np.zeros([num_episodes, 1000])\n",
        "    weight_list2 = np.zeros([num_episodes, 1002])\n",
        "    weight_list3 = np.zeros([num_episodes, 1002])\n",
        "    marginal_weight = np.zeros([num_episodes, 1000])\n",
        "    marginal_weight_2 = np.zeros([num_episodes, 1000])\n",
        "    auxi_list = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list2 = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list2_2 = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list_2 = np.zeros([num_episodes, 1000])\n",
        "    auxi_list2 = np.zeros([num_episodes, 1000])\n",
        "    reward_list = np.zeros([num_episodes, 1000])\n",
        "    state_list = np.zeros([num_episodes, 1000])\n",
        "    action_list = np.zeros([num_episodes, 1000])\n",
        "\n",
        "    count_list = np.zeros(1000)\n",
        "    episolode_longe_list = []\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        if i_episode % 200 == 0:\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        episode = episode_episode[i_episode]\n",
        "\n",
        "        W = 1.0\n",
        "        W_list = []\n",
        "        episolode_longe_list.append(len(episode))\n",
        "\n",
        "        weight_list2[i_episode, 0] = 1.0\n",
        "        for t in range(len(episode)):\n",
        "            state, action, reward = episode[t]\n",
        "            reward_list[i_episode, t] = reward\n",
        "            state_list[i_episode, t] = state\n",
        "            action_list[i_episode, t] = action\n",
        "\n",
        "            W = W*target_dense(state)[action]/behav_dense(state)[action]*discount_factor\n",
        "            probprob = 0.9*Q_space[state,:] + 0.1*prob1\n",
        "            W_list.append(W)\n",
        "            weight_list[i_episode, t] = W_list[t]\n",
        "            weight_list2[i_episode, t+1] = W_list[t]\n",
        "            weight_list3[i_episode, t] = target_dense(state)[action]/behav_dense(state)[action]\n",
        "\n",
        "            count_list[t] += 1.0\n",
        "\n",
        "            if t==0:\n",
        "                auxi_list[i_episode, t] = W_list[t]*Q_[state, action]-np.sum(probprob*Q_[state,:])\n",
        "            else:\n",
        "                auxi_list[i_episode, t] = W_list[t]*Q_[state, action]-W_list[t-1]*np.sum(probprob*Q_[state,:])\n",
        "\n",
        "            if t==0:\n",
        "                auxi_list2[i_episode, t] = W_list[t]-1.0\n",
        "            else:\n",
        "                auxi_list2[i_episode, t] = W_list[t]-W_list[t-1]\n",
        "\n",
        "    print(np.max(np.array(episolode_longe_list)))\n",
        "\n",
        "    weight_list_mean = np.mean(weight_list, 1)\n",
        "    reward_list_mean = np.mean(reward_list, 1)\n",
        "    auxi_list_mean = np.mean(auxi_list, 1)\n",
        "    auxi_list2_mean = np.mean(auxi_list2, 1)\n",
        "\n",
        "    val = []\n",
        "\n",
        "    ##### IPW - Standard Importance Sampling\n",
        "    for i in range(num_episodes):\n",
        "        predic_list.append(np.sum(weight_list[i,:]*reward_list[i,:]))\n",
        "\n",
        "    val.append(np.mean(predic_list))\n",
        "\n",
        "    #### Marginalized-IPW\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        for j in range(episolode_longe_list[i]):\n",
        "            marginal_weight[i,j] = np.mean(weight_list[:,j][(state_list[:,j]==state_list[i,j]) & (action_list[:,j]==action_list[i,j])])\n",
        "            if j==0:\n",
        "                marginal_weight_2[i,j] = weight_list3[i,j]\n",
        "            else:\n",
        "                marginal_weight_2[i,j] = np.mean(weight_list[:,j-1][(state_list[:,j]==state_list[i,j])])*weight_list3[i,j]\n",
        "\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        for t in range(episolode_longe_list[i_episode]):\n",
        "            state = int(state_list[i_episode,t])  # Using int instead of np.int for Python 3\n",
        "            action = int(action_list[i_episode,t])  # Using int instead of np.int for Python 3\n",
        "            probprob = 0.9*Q_space[state,:] + 0.1*prob1\n",
        "            if t==0:\n",
        "                marginal_auxi_list[i_episode,t] = marginal_weight[i_episode,t]*Q_[state,action]-np.sum(probprob*Q_[state,:])\n",
        "                marginal_auxi_list_2[i_episode,t] = marginal_weight_2[i_episode,t]*Q_[state,action]-np.sum(probprob*Q_[state,:])\n",
        "                auxi_list[i_episode,t] = weight_list[i_episode,t]*Q_[state,action]-np.sum(probprob*Q_[state,:])\n",
        "            else:\n",
        "                marginal_auxi_list[i_episode,t] = marginal_weight[i_episode,t]*(Q_[state,action])-marginal_weight[i_episode,t-1]*np.sum(probprob*(Q_[state,:]))\n",
        "                marginal_auxi_list_2[i_episode,t] = marginal_weight_2[i_episode,t]*(Q_[state,action])-marginal_weight_2[i_episode,t-1]*np.sum(probprob*(Q_[state,:]))\n",
        "                auxi_list[i_episode,t] = weight_list[i_episode,t]*(Q_[state,action])-weight_list[i_episode,t-1]*np.sum(probprob*(Q_[state,:]))\n",
        "\n",
        "            if t==0:\n",
        "                marginal_auxi_list2[i_episode,t] = marginal_weight[i_episode,t]-1.0\n",
        "                marginal_auxi_list2_2[i_episode,t] = marginal_weight_2[i_episode,t]-1.0\n",
        "                auxi_list2[i_episode,t] = weight_list[i_episode,t]-1.0\n",
        "            else:\n",
        "                marginal_auxi_list2[i_episode,t] =  marginal_weight[i_episode,t]- marginal_weight[i_episode,t-1]\n",
        "                marginal_auxi_list2_2[i_episode,t] =  marginal_weight_2[i_episode,t]- marginal_weight_2[i_episode,t-1]\n",
        "                auxi_list2[i_episode,t] = weight_list[i_episode,t]-weight_list[i_episode,t-1]\n",
        "\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        predic_list2.append(np.sum(marginal_weight[i,:]*reward_list[i,:]))\n",
        "\n",
        "    ### marginal ipw2  #### Using action and state\n",
        "    val.append(np.mean(predic_list2))\n",
        "\n",
        "\n",
        "    ### marginal ipw3#### Using only state\n",
        "    for i in range(num_episodes):\n",
        "        predic_list22.append(np.sum(marginal_weight_2[i,:]*reward_list[i,:]))\n",
        "\n",
        "    val.append(np.mean(predic_list22))\n",
        "\n",
        "\n",
        "    #### DR\n",
        "    val.append(np.mean(predic_list)-np.mean(np.sum(auxi_list,1)))\n",
        "\n",
        "    #### marginal DR 1  #### Using action and state\n",
        "    val.append(np.mean(predic_list2)-np.mean(np.sum(marginal_auxi_list,1)))\n",
        "    #### marginal DR 2   #### Using only state\n",
        "    val.append(np.mean(predic_list22)-np.mean(np.sum(marginal_auxi_list_2,1)))\n",
        "\n",
        "    return val\n",
        "\n",
        "# Main experiment run - with sample splitting like the original\n",
        "is_list = []\n",
        "is2_list = []\n",
        "is3_list = []\n",
        "wis_list = []\n",
        "wis2_list = []\n",
        "dm_list = []\n",
        "dr_list = []\n",
        "dr2_list = []\n",
        "dr3_list = []\n",
        "bdr_list = []\n",
        "drs_list = []\n",
        "drs2_list = []\n",
        "drss_list = []\n",
        "mdr_list = []\n",
        "mdr_list2 = []\n",
        "\n",
        "sample_size = 1000\n",
        "# In Python 3, integer division requires // instead of /\n",
        "sample_size = sample_size // 2\n",
        "\n",
        "for kkk in range(100):\n",
        "    print(kkk)\n",
        "    #### Sample splitting\n",
        "    ### First fold\n",
        "    predicted_Q, episode_episode = sarsa2(env, sample_policy, behavior_policy, sample_size)\n",
        "    V_10k_1 = mc_prediction(env, sample_policy, behavior_policy, episode_episode, predicted_Q, num_episodes=sample_size)\n",
        "\n",
        "    ### Second fold\n",
        "    predicted_Q, episode_episode = sarsa2(env, sample_policy, behavior_policy, sample_size)\n",
        "    V_10k_2 = mc_prediction(env, sample_policy, behavior_policy, episode_episode, predicted_Q, num_episodes=sample_size)\n",
        "\n",
        "    V_10k = 0.5*(np.array(V_10k_1)+np.array(V_10k_2))\n",
        "    is_list.append(np.mean(V_10k[0]))\n",
        "    is2_list.append(np.mean(V_10k[1]))\n",
        "    is3_list.append(np.mean(V_10k[2]))\n",
        "    dr_list.append(np.mean(V_10k[3]))\n",
        "    dr2_list.append(np.mean(V_10k[4]))\n",
        "    dr3_list.append(np.mean(V_10k[5]))\n",
        "    probprob = 0.9*Q_space[36,:] + 0.1*prob1\n",
        "    dm_list.append(np.sum(probprob*predicted_Q[36,:]))\n",
        "\n",
        "    # Save results periodically\n",
        "    if (kkk + 1) % 10 == 0:\n",
        "        np.savez(f\"2estimator_list_ipw_{betabeta}_{sample_size}\", a=np.array(is_list))\n",
        "        np.savez(f\"2estimator_list_ipw2_{betabeta}_{sample_size}\", a=np.array(is3_list))\n",
        "        np.savez(f\"2estimator_list_dm_{betabeta}_{sample_size}\", a=np.array(dm_list))\n",
        "        np.savez(f\"2estimator_list_dr_{betabeta}_{sample_size}\", a=np.array(dr_list))\n",
        "        np.savez(f\"2estimator_list_dr2_{betabeta}_{sample_size}\", a=np.array(dr3_list))\n",
        "\n",
        "# Analysis of results\n",
        "true = -42.49\n",
        "def mse(aaa):\n",
        "    \"\"\"Calculate the Mean Squared Error correctly for comparison\"\"\"\n",
        "    aaa = np.array(aaa)\n",
        "    # Filter extreme values\n",
        "    aaa = aaa[aaa > -100]\n",
        "    # Original MSE calculation\n",
        "    return [np.mean((((aaa-true)*(aaa-true)))), np.sqrt(np.var((aaa-true)*(aaa-true)))]\n",
        "\n",
        "print(\"IPW:\")\n",
        "print(f\"Mean: {np.mean(is_list)}\")\n",
        "print(f\"MSE: {mse(is_list)}\")\n",
        "\n",
        "print(\"WIS:\")\n",
        "print(f\"Mean: {np.mean(is3_list)}\")  # Note: Original used is3_list for WIS\n",
        "print(f\"MSE: {mse(is3_list)}\")\n",
        "\n",
        "print(\"DM:\")\n",
        "print(f\"Mean: {np.mean(dm_list)}\")\n",
        "print(f\"MSE: {mse(dm_list)}\")\n",
        "\n",
        "print(\"DR:\")\n",
        "print(f\"Mean: {np.mean(dr_list)}\")\n",
        "print(f\"MSE: {mse(dr_list)}\")\n",
        "\n",
        "print(\"DR3:\")\n",
        "print(f\"Mean: {np.mean(dr3_list)}\")\n",
        "print(f\"MSE: {mse(dr3_list)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnQ9THg00_XF",
        "outputId": "01d3aff0-63b4-42dc-9a9e-04763360bf2e"
      },
      "id": "tnQ9THg00_XF",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "282\n",
            "307\n",
            "1\n",
            "215\n",
            "249\n",
            "2\n",
            "215\n",
            "287\n",
            "3\n",
            "313\n",
            "221\n",
            "4\n",
            "183\n",
            "282\n",
            "5\n",
            "201\n",
            "237\n",
            "6\n",
            "196\n",
            "250\n",
            "7\n",
            "220\n",
            "164\n",
            "8\n",
            "307\n",
            "289\n",
            "9\n",
            "269\n",
            "444\n",
            "10\n",
            "219\n",
            "226\n",
            "11\n",
            "216\n",
            "271\n",
            "12\n",
            "246\n",
            "235\n",
            "13\n",
            "214\n",
            "220\n",
            "14\n",
            "207\n",
            "216\n",
            "15\n",
            "224\n",
            "252\n",
            "16\n",
            "241\n",
            "237\n",
            "17\n",
            "260\n",
            "246\n",
            "18\n",
            "246\n",
            "194\n",
            "19\n",
            "335\n",
            "265\n",
            "20\n",
            "259\n",
            "212\n",
            "21\n",
            "225\n",
            "321\n",
            "22\n",
            "215\n",
            "222\n",
            "23\n",
            "190\n",
            "233\n",
            "24\n",
            "241\n",
            "340\n",
            "25\n",
            "217\n",
            "212\n",
            "26\n",
            "274\n",
            "223\n",
            "27\n",
            "235\n",
            "231\n",
            "28\n",
            "201\n",
            "224\n",
            "29\n",
            "172\n",
            "215\n",
            "30\n",
            "195\n",
            "247\n",
            "31\n",
            "209\n",
            "299\n",
            "32\n",
            "221\n",
            "215\n",
            "33\n",
            "270\n",
            "219\n",
            "34\n",
            "245\n",
            "232\n",
            "35\n",
            "231\n",
            "224\n",
            "36\n",
            "243\n",
            "212\n",
            "37\n",
            "243\n",
            "273\n",
            "38\n",
            "271\n",
            "202\n",
            "39\n",
            "309\n",
            "184\n",
            "40\n",
            "230\n",
            "247\n",
            "41\n",
            "265\n",
            "172\n",
            "42\n",
            "227\n",
            "252\n",
            "43\n",
            "190\n",
            "207\n",
            "44\n",
            "253\n",
            "316\n",
            "45\n",
            "214\n",
            "283\n",
            "46\n",
            "263\n",
            "195\n",
            "47\n",
            "236\n",
            "208\n",
            "48\n",
            "301\n",
            "329\n",
            "49\n",
            "200\n",
            "266\n",
            "50\n",
            "267\n",
            "264\n",
            "51\n",
            "297\n",
            "216\n",
            "52\n",
            "273\n",
            "206\n",
            "53\n",
            "314\n",
            "247\n",
            "54\n",
            "241\n",
            "227\n",
            "55\n",
            "192\n",
            "276\n",
            "56\n",
            "323\n",
            "392\n",
            "57\n",
            "174\n",
            "204\n",
            "58\n",
            "257\n",
            "182\n",
            "59\n",
            "275\n",
            "200\n",
            "60\n",
            "213\n",
            "191\n",
            "61\n",
            "220\n",
            "235\n",
            "62\n",
            "241\n",
            "244\n",
            "63\n",
            "261\n",
            "674\n",
            "64\n",
            "257\n",
            "258\n",
            "65\n",
            "231\n",
            "258\n",
            "66\n",
            "254\n",
            "264\n",
            "67\n",
            "298\n",
            "176\n",
            "68\n",
            "233\n",
            "197\n",
            "69\n",
            "209\n",
            "192\n",
            "70\n",
            "338\n",
            "188\n",
            "71\n",
            "304\n",
            "202\n",
            "72\n",
            "239\n",
            "182\n",
            "73\n",
            "284\n",
            "205\n",
            "74\n",
            "186\n",
            "318\n",
            "75\n",
            "265\n",
            "194\n",
            "76\n",
            "172\n",
            "312\n",
            "77\n",
            "221\n",
            "296\n",
            "78\n",
            "197\n",
            "230\n",
            "79\n",
            "252\n",
            "183\n",
            "80\n",
            "254\n",
            "260\n",
            "81\n",
            "261\n",
            "206\n",
            "82\n",
            "329\n",
            "241\n",
            "83\n",
            "227\n",
            "183\n",
            "84\n",
            "230\n",
            "282\n",
            "85\n",
            "180\n",
            "235\n",
            "86\n",
            "182\n",
            "402\n",
            "87\n",
            "162\n",
            "240\n",
            "88\n",
            "288\n",
            "194\n",
            "89\n",
            "214\n",
            "194\n",
            "90\n",
            "335\n",
            "198\n",
            "91\n",
            "277\n",
            "254\n",
            "92\n",
            "189\n",
            "172\n",
            "93\n",
            "326\n",
            "193\n",
            "94\n",
            "232\n",
            "273\n",
            "95\n",
            "207\n",
            "227\n",
            "96\n",
            "178\n",
            "198\n",
            "97\n",
            "195\n",
            "220\n",
            "98\n",
            "274\n",
            "203\n",
            "99\n",
            "211\n",
            "333\n",
            "IPW:\n",
            "Mean: -54.75616680808431\n",
            "MSE: [np.float64(160.30981785049113), np.float64(84.892121100215)]\n",
            "WIS:\n",
            "Mean: -53.458516161830985\n",
            "MSE: [np.float64(127.63291481174467), np.float64(64.21425268143648)]\n",
            "DM:\n",
            "Mean: -52.8702278720724\n",
            "MSE: [np.float64(107.83228360781379), np.float64(5.982560508806721)]\n",
            "DR:\n",
            "Mean: -55.37211197782537\n",
            "MSE: [np.float64(166.0145781196328), np.float64(6.663141640878914)]\n",
            "DR3:\n",
            "Mean: -55.261490392640056\n",
            "MSE: [np.float64(163.16388325845435), np.float64(5.914647204533788)]\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-10T18:26:05.270719Z",
          "start_time": "2025-05-10T18:18:20.420269Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "532a8fd56a713ebe",
        "outputId": "3aef18e4-ae52-4d5f-ca36-a2d0ac90f3bf"
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import sys\n",
        "import gym\n",
        "\n",
        "# Since lib.envs isn't available, we'll need to define these environments here\n",
        "# or use gym environments directly. For now, I'll create simplified versions.\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Simple CliffWalkingEnv implementation\n",
        "class CliffWalkingEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.shape = (4, 12)\n",
        "        self.start_state_index = np.ravel_multi_index((3, 0), self.shape)\n",
        "        self.goal_state_index = np.ravel_multi_index((3, 11), self.shape)\n",
        "        self.cliff = list(range(np.ravel_multi_index((3, 1), self.shape),\n",
        "                               np.ravel_multi_index((3, 11), self.shape)))\n",
        "        self.nS = self.shape[0] * self.shape[1]\n",
        "        self.nA = 4  # up, right, down, left\n",
        "\n",
        "        # Calculate transition probabilities and rewards\n",
        "        self.P = {}\n",
        "        for s in range(self.nS):\n",
        "            position = np.unravel_index(s, self.shape)\n",
        "            self.P[s] = {a: [] for a in range(self.nA)}\n",
        "\n",
        "            # Actions: 0=up, 1=right, 2=down, 3=left\n",
        "            for a in range(self.nA):\n",
        "                reward = -1.0  # default reward for each move\n",
        "                next_position = list(position)\n",
        "                if a == 0:\n",
        "                    next_position[0] = max(position[0] - 1, 0)\n",
        "                elif a == 1:\n",
        "                    next_position[1] = min(position[1] + 1, self.shape[1] - 1)\n",
        "                elif a == 2:\n",
        "                    next_position[0] = min(position[0] + 1, self.shape[0] - 1)\n",
        "                elif a == 3:\n",
        "                    next_position[1] = max(position[1] - 1, 0)\n",
        "\n",
        "                next_state = np.ravel_multi_index(next_position, self.shape)\n",
        "\n",
        "                # Check if we're at the cliff\n",
        "                if s in self.cliff:\n",
        "                    next_state = self.start_state_index\n",
        "                    reward = -100.0\n",
        "\n",
        "                # Check if we're at the goal\n",
        "                done = next_state == self.goal_state_index\n",
        "\n",
        "                self.P[s][a] = [(1.0, next_state, reward, done)]\n",
        "\n",
        "        self.observation_space = gym.spaces.Discrete(self.nS)\n",
        "        self.action_space = gym.spaces.Discrete(self.nA)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done, _ = self._step(action)\n",
        "        self.s = state\n",
        "        return (state, reward, done, {})\n",
        "\n",
        "    def _step(self, action):\n",
        "        (probs, next_state, reward, done) = self.P[self.s][action][0]\n",
        "        return (next_state, reward, done, {})\n",
        "\n",
        "    def reset(self):\n",
        "        self.s = self.start_state_index\n",
        "        return self.s\n",
        "\n",
        "# Simple WindyGridworldEnv implementation (not fully used in this code)\n",
        "class WindyGridworldEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.shape = (7, 10)\n",
        "        self.nS = self.shape[0] * self.shape[1]\n",
        "        self.nA = 4  # up, right, down, left\n",
        "        self.wind = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        # Not implemented as it's not used in the main code\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        self.s = np.ravel_multi_index((3, 0), self.shape)\n",
        "        return self.s\n",
        "\n",
        "from scipy.optimize import minimize, rosen, rosen_der\n",
        "from scipy.optimize import Bounds\n",
        "\n",
        "bounds = Bounds([-0.1, -0.1], [0.1, 0.1])\n",
        "\n",
        "env = CliffWalkingEnv()\n",
        "\n",
        "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
        "    def policy_fn(observation):\n",
        "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
        "        best_action = np.argmax(Q[observation])\n",
        "        A[best_action] += (1.0 - epsilon)\n",
        "        return A\n",
        "    return policy_fn\n",
        "\n",
        "Q_space = np.load(\"/content/drive/MyDrive/Work/Estimators/DoubleReinforcement/Q-table-cliff.npz\")[\"xxx\"]\n",
        "Q_space2 = np.load(\"/content/drive/MyDrive/Work/Estimators/DoubleReinforcement/Q-table-real-cliff.npz\")[\"xxx\"] #Q-table-cliff.npz\n",
        "\n",
        "prob1 = [1.0 for i in range((env.nA))]\n",
        "prob1 = prob1/np.sum(prob1)\n",
        "\n",
        "betabeta = 0.8\n",
        "def sample_policy(observation, alpha=0.9):\n",
        "    prob2 = alpha*Q_space[observation,:] + (1-alpha)*prob1\n",
        "    return np.random.choice(env.nA, 1, p=prob2)[0]\n",
        "\n",
        "\n",
        "def behavior_policy(observation, beta=betabeta):\n",
        "    prob2 = beta*Q_space[observation,:] + (1-beta)*prob1\n",
        "    return np.random.choice(env.nA, 1, p=prob2)[0]\n",
        "\n",
        "\n",
        "def target_dense(observation, alpha=0.9):\n",
        "    prob2 = alpha*Q_space[observation,:] + (1-alpha)*prob1\n",
        "    return prob2\n",
        "\n",
        "def behav_dense(observation, beta=betabeta):\n",
        "    prob2 = beta*Q_space[observation,:] + (1-beta)*prob1\n",
        "    return prob2\n",
        "\n",
        "def sarsa2(env, policy, policy2, num_episodes, discount_factor=1.0, Q_space2=Q_space2, alpha=0.6, epsilon=0.03):\n",
        "\n",
        "    Q = np.copy(Q_space2)\n",
        "    episode_episode = []\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "\n",
        "        if (i_episode + 1) % 200 == 0:\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        state = env.reset()\n",
        "        action = policy2(state)\n",
        "\n",
        "        episode = []\n",
        "\n",
        "        for t in itertools.count():\n",
        "            # Take a step\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            # Pick the next action\n",
        "            next_action = policy2(next_state)\n",
        "\n",
        "            # TD Update\n",
        "            td_target = reward + discount_factor * np.sum(Q[next_state,:]*target_dense(next_state))\n",
        "            td_delta = td_target - Q[state, action]\n",
        "            Q[state, action] += alpha * td_delta\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            action = next_action\n",
        "            state = next_state\n",
        "\n",
        "        episode_episode.append(episode)\n",
        "\n",
        "    return Q, episode_episode\n",
        "\n",
        "bounds = Bounds([-0.2, -0.2], [0.2, 0.2])\n",
        "def sigmoid(x, derivative=False):\n",
        "    return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
        "\n",
        "\n",
        "depth = 1\n",
        "def mc_prediction(env, policy, policy2, episode_episode, Q_=1.0, num_episodes=100, discount_factor=1.0):\n",
        "\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "    returns_count2 = defaultdict(float)\n",
        "\n",
        "    predic_list = []\n",
        "    predic_list2 = []\n",
        "    predic_list3 = []\n",
        "    predic_list22 = []\n",
        "    predic_list4 = []\n",
        "    predic_list5 = np.ones(num_episodes)\n",
        "    auxiauxi = []\n",
        "    epiepi = []\n",
        "    weight_list = np.zeros([num_episodes, 1000])  # For bounded IPW\n",
        "    weight_list2 = np.zeros([num_episodes, 1002])  # For bounded IPW\n",
        "    weight_list3 = np.zeros([num_episodes, 1002])  # For bounded IPW\n",
        "    marginal_weight = np.zeros([num_episodes, 1000])  # For bounded IPW\n",
        "    marginal_weight_2 = np.zeros([num_episodes, 1000])  # For bounded IPW\n",
        "    auxi_list = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list2 = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list2_2 = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list_2 = np.zeros([num_episodes, 1000])\n",
        "    auxi_list2 = np.zeros([num_episodes, 1000])\n",
        "    reward_list = np.zeros([num_episodes, 1000])\n",
        "    state_list = np.zeros([num_episodes, 1000])\n",
        "    action_list = np.zeros([num_episodes, 1000])\n",
        "\n",
        "    count_list = np.zeros(1000)\n",
        "    episolode_longe_list = []\n",
        "\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "\n",
        "        if i_episode % 200 == 0:\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        episode = episode_episode[i_episode]\n",
        "\n",
        "        W = 1.0\n",
        "        W_list = []\n",
        "        episolode_longe_list.append(len(episode))\n",
        "\n",
        "        weight_list2[i_episode, 0] = 1.0\n",
        "        for t in range(len(episode)):\n",
        "            state, action, reward = episode[t]\n",
        "            reward_list[i_episode, t] = reward\n",
        "            state_list[i_episode, t] = state\n",
        "            action_list[i_episode, t] = action\n",
        "\n",
        "            W = W*target_dense(state)[action]/behav_dense(state)[action]*discount_factor\n",
        "            probprob = 0.9*Q_space[state,:] + 0.1*prob1\n",
        "            W_list.append(W)\n",
        "            weight_list[i_episode, t] = W_list[t]\n",
        "            weight_list2[i_episode, t+1] = W_list[t]\n",
        "            weight_list3[i_episode, t] = target_dense(state)[action]/behav_dense(state)[action]\n",
        "\n",
        "            count_list[t] += 1.0\n",
        "\n",
        "            if t==0:\n",
        "                auxi_list[i_episode, t] = W_list[t]*Q_[state, action]-np.sum(probprob*Q_[state,:])\n",
        "            else:\n",
        "                auxi_list[i_episode, t] = W_list[t]*Q_[state, action]-W_list[t-1]*np.sum(probprob*Q_[state,:])\n",
        "\n",
        "            if t==0:\n",
        "                auxi_list2[i_episode, t] = W_list[t]-1.0\n",
        "            else:\n",
        "                auxi_list2[i_episode, t] = W_list[t]-W_list[t-1]\n",
        "\n",
        "    print(np.max(np.array(episolode_longe_list)))\n",
        "\n",
        "\n",
        "    weight_list_mean = np.mean(weight_list, 1)\n",
        "    reward_list_mean = np.mean(reward_list, 1)\n",
        "    auxi_list_mean = np.mean(auxi_list, 1)\n",
        "    auxi_list2_mean = np.mean(auxi_list2, 1)\n",
        "\n",
        "    val = []\n",
        "\n",
        "    ##### IPW\n",
        "    for i in range(num_episodes):\n",
        "        predic_list.append(np.sum(weight_list[i,:]*reward_list[i,:]))\n",
        "\n",
        "    val.append(np.mean(predic_list))\n",
        "\n",
        "    #### Marginalized-IPW\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        for j in range(episolode_longe_list[i]):\n",
        "            marginal_weight[i,j] = np.mean(weight_list[:,j][(state_list[:,j]==state_list[i,j]) & (action_list[:,j]==action_list[i,j])])\n",
        "            if j==0:\n",
        "                marginal_weight_2[i,j] = weight_list3[i,j]\n",
        "            else:\n",
        "                marginal_weight_2[i,j] = np.mean(weight_list[:,j-1][(state_list[:,j]==state_list[i,j])])*weight_list3[i,j]\n",
        "\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        for t in range(episolode_longe_list[i_episode]):\n",
        "            state = int(state_list[i_episode, t])  # Changed np.int to int\n",
        "            action = int(action_list[i_episode, t])  # Changed np.int to int\n",
        "            probprob = 0.9*Q_space[state,:] + 0.1*prob1\n",
        "            if t==0:\n",
        "                marginal_auxi_list[i_episode, t] = marginal_weight[i_episode, t]*Q_[state, action]-np.sum(probprob*Q_[state,:])\n",
        "                marginal_auxi_list_2[i_episode, t] = marginal_weight_2[i_episode, t]*Q_[state, action]-np.sum(probprob*Q_[state,:])\n",
        "                auxi_list[i_episode, t] = weight_list[i_episode, t]*Q_[state, action]-np.sum(probprob*Q_[state,:])\n",
        "            else:\n",
        "                marginal_auxi_list[i_episode, t] = marginal_weight[i_episode, t]*(Q_[state, action])-marginal_weight[i_episode, t-1]*np.sum(probprob*(Q_[state,:]))\n",
        "                marginal_auxi_list_2[i_episode, t] = marginal_weight_2[i_episode, t]*(Q_[state, action])-marginal_weight_2[i_episode, t-1]*np.sum(probprob*(Q_[state,:]))\n",
        "                auxi_list[i_episode, t] = weight_list[i_episode, t]*(Q_[state, action])-weight_list[i_episode, t-1]*np.sum(probprob*(Q_[state,:]))\n",
        "\n",
        "            if t==0:\n",
        "                marginal_auxi_list2[i_episode, t] = marginal_weight[i_episode, t]-1.0\n",
        "                marginal_auxi_list2_2[i_episode, t] = marginal_weight_2[i_episode, t]-1.0\n",
        "                auxi_list2[i_episode, t] = weight_list[i_episode, t]-1.0\n",
        "            else:\n",
        "                marginal_auxi_list2[i_episode, t] = marginal_weight[i_episode, t]- marginal_weight[i_episode, t-1]\n",
        "                marginal_auxi_list2_2[i_episode, t] = marginal_weight_2[i_episode, t]- marginal_weight_2[i_episode, t-1]\n",
        "                auxi_list2[i_episode, t] = weight_list[i_episode, t]-weight_list[i_episode, t-1]\n",
        "\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        predic_list2.append(np.sum(marginal_weight[i,:]*reward_list[i,:]))\n",
        "\n",
        "    ### marginal ipw2  #### Using action and state\n",
        "    val.append(np.mean(predic_list2))\n",
        "\n",
        "\n",
        "    ### marginal ipw3#### Using only state\n",
        "    for i in range(num_episodes):\n",
        "        predic_list22.append(np.sum(marginal_weight_2[i,:]*reward_list[i,:]))\n",
        "\n",
        "    val.append(np.mean(predic_list22))\n",
        "\n",
        "\n",
        "    #### DR\n",
        "    val.append(np.mean(predic_list)-np.mean(np.sum(auxi_list, 1)))\n",
        "\n",
        "    #### marginal DR 1  #### Using action and state\n",
        "    val.append(np.mean(predic_list2)-np.mean(np.sum(marginal_auxi_list, 1)))\n",
        "    #### marginal DR 2   #### Using only state\n",
        "    val.append(np.mean(predic_list22)-np.mean(np.sum(marginal_auxi_list_2, 1)))\n",
        "\n",
        "    return val\n",
        "\n",
        "# Main experiment run\n",
        "is_list = []\n",
        "is2_list = []\n",
        "is3_list = []\n",
        "wis_list = []\n",
        "wis2_list = []\n",
        "dm_list = []\n",
        "dr_list = []\n",
        "dr2_list = []\n",
        "dr3_list = []\n",
        "bdr_list = []\n",
        "drs_list = []\n",
        "drs2_list = []\n",
        "drss_list = []\n",
        "mdr_list = []\n",
        "mdr_list2 = []\n",
        "\n",
        "sample_size = 1000\n",
        "sample_size = sample_size // 2  # Integer division in Python 3\n",
        "for kkk in range(100):\n",
        "    print(kkk)\n",
        "    #### Sample splititng\n",
        "    ### First fold\n",
        "\n",
        "    predicted_Q, episode_episode = sarsa2(env, sample_policy, behavior_policy, sample_size)\n",
        "    V_10k_1 = mc_prediction(env, sample_policy, behavior_policy, episode_episode, predicted_Q, num_episodes=sample_size)\n",
        "\n",
        "    ### Second fold\n",
        "    predicted_Q, episode_episode = sarsa2(env, sample_policy, behavior_policy, sample_size)\n",
        "    V_10k_2 = mc_prediction(env, sample_policy, behavior_policy, episode_episode, predicted_Q, num_episodes=sample_size)\n",
        "\n",
        "    V_10k = 0.5*(np.array(V_10k_1)+np.array(V_10k_2))\n",
        "    is_list.append(np.mean(V_10k[0]))\n",
        "    is2_list.append(np.mean(V_10k[1]))\n",
        "    is3_list.append(np.mean(V_10k[2]))\n",
        "    dr_list.append(np.mean(V_10k[3]))\n",
        "    dr2_list.append(np.mean(V_10k[4]))\n",
        "    dr3_list.append(np.mean(V_10k[5]))\n",
        "    probprob = 0.9*Q_space[36,:] + 0.1*prob1\n",
        "    dm_list.append(np.sum(probprob*predicted_Q[36,:]))\n",
        "    np.savez(\"2estimator_list_ipw_\"+str(betabeta)+\"_\"+str(sample_size), a=is_list)\n",
        "    np.savez(\"2estimator_list_ipw2_\"+str(betabeta)+\"_\"+str(sample_size), a=is3_list)\n",
        "    np.savez(\"2estimator_list_dm_\"+str(betabeta)+\"_\"+str(sample_size), a=dm_list)\n",
        "    np.savez(\"2estimator_list_dr_\"+str(betabeta)+\"_\"+str(sample_size), a=dr_list)\n",
        "    np.savez(\"2estimator_list_dr2_\"+str(betabeta)+\"_\"+str(sample_size), a=dr3_list)\n",
        "\n",
        "# Analysis of results\n",
        "true = -42.49\n",
        "\n",
        "# FIX: Properly calculate MSE instead of using hardcoded values\n",
        "def mse(aaa):\n",
        "    aaa = np.array(aaa)\n",
        "    aaa = aaa[aaa>-100]  # Filter extreme values\n",
        "    mean_val = np.mean(aaa)  # Calculate mean\n",
        "    bias = mean_val - true  # Calculate bias\n",
        "    bias_squared = bias * bias  # Square the bias\n",
        "    variance = np.var(aaa)  # Calculate variance\n",
        "    mse_value = bias_squared + variance  # MSE = bias² + variance\n",
        "    return [mse_value, np.sqrt(np.var((aaa-true)*(aaa-true)))]  # Return MSE and RMSE\n",
        "\n",
        "print(np.mean(is_list))\n",
        "print(mse(is_list))\n",
        "print(\"wis\")\n",
        "print(np.mean(is3_list))\n",
        "print(mse(is3_list))\n",
        "print(\"dm\")\n",
        "print(np.mean(dm_list))\n",
        "print(mse(dm_list))\n",
        "print(\"dr\")\n",
        "print(np.mean(dr_list))\n",
        "print(mse(dr_list))\n",
        "print(\"dr3\")\n",
        "print(np.mean(dr3_list))\n",
        "print(mse(dr3_list))"
      ],
      "id": "532a8fd56a713ebe",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "237\n",
            "201\n",
            "1\n",
            "293\n",
            "251\n",
            "2\n",
            "269\n",
            "269\n",
            "3\n",
            "307\n",
            "382\n",
            "4\n",
            "262\n",
            "190\n",
            "5\n",
            "232\n",
            "196\n",
            "6\n",
            "232\n",
            "291\n",
            "7\n",
            "316\n",
            "241\n",
            "8\n",
            "224\n",
            "206\n",
            "9\n",
            "274\n",
            "274\n",
            "10\n",
            "261\n",
            "260\n",
            "11\n",
            "250\n",
            "254\n",
            "12\n",
            "196\n",
            "260\n",
            "13\n",
            "324\n",
            "285\n",
            "14\n",
            "259\n",
            "250\n",
            "15\n",
            "235\n",
            "198\n",
            "16\n",
            "267\n",
            "234\n",
            "17\n",
            "250\n",
            "233\n",
            "18\n",
            "205\n",
            "190\n",
            "19\n",
            "196\n",
            "208\n",
            "20\n",
            "340\n",
            "200\n",
            "21\n",
            "233\n",
            "269\n",
            "22\n",
            "228\n",
            "218\n",
            "23\n",
            "246\n",
            "266\n",
            "24\n",
            "238\n",
            "288\n",
            "25\n",
            "248\n",
            "424\n",
            "26\n",
            "254\n",
            "258\n",
            "27\n",
            "240\n",
            "275\n",
            "28\n",
            "213\n",
            "194\n",
            "29\n",
            "202\n",
            "251\n",
            "30\n",
            "174\n",
            "256\n",
            "31\n",
            "227\n",
            "252\n",
            "32\n",
            "182\n",
            "388\n",
            "33\n",
            "212\n",
            "282\n",
            "34\n",
            "333\n",
            "242\n",
            "35\n",
            "214\n",
            "198\n",
            "36\n",
            "245\n",
            "234\n",
            "37\n",
            "218\n",
            "245\n",
            "38\n",
            "258\n",
            "229\n",
            "39\n",
            "236\n",
            "303\n",
            "40\n",
            "229\n",
            "219\n",
            "41\n",
            "265\n",
            "169\n",
            "42\n",
            "217\n",
            "186\n",
            "43\n",
            "222\n",
            "158\n",
            "44\n",
            "374\n",
            "243\n",
            "45\n",
            "290\n",
            "264\n",
            "46\n",
            "291\n",
            "219\n",
            "47\n",
            "260\n",
            "220\n",
            "48\n",
            "210\n",
            "217\n",
            "49\n",
            "259\n",
            "199\n",
            "50\n",
            "190\n",
            "209\n",
            "51\n",
            "238\n",
            "211\n",
            "52\n",
            "217\n",
            "208\n",
            "53\n",
            "309\n",
            "209\n",
            "54\n",
            "159\n",
            "241\n",
            "55\n",
            "184\n",
            "326\n",
            "56\n",
            "204\n",
            "243\n",
            "57\n",
            "188\n",
            "201\n",
            "58\n",
            "249\n",
            "278\n",
            "59\n",
            "238\n",
            "210\n",
            "60\n",
            "179\n",
            "234\n",
            "61\n",
            "188\n",
            "211\n",
            "62\n",
            "156\n",
            "177\n",
            "63\n",
            "220\n",
            "261\n",
            "64\n",
            "177\n",
            "244\n",
            "65\n",
            "247\n",
            "234\n",
            "66\n",
            "394\n",
            "214\n",
            "67\n",
            "226\n",
            "221\n",
            "68\n",
            "200\n",
            "208\n",
            "69\n",
            "537\n",
            "306\n",
            "70\n",
            "196\n",
            "214\n",
            "71\n",
            "237\n",
            "225\n",
            "72\n",
            "205\n",
            "270\n",
            "73\n",
            "207\n",
            "226\n",
            "74\n",
            "322\n",
            "196\n",
            "75\n",
            "305\n",
            "251\n",
            "76\n",
            "276\n",
            "232\n",
            "77\n",
            "193\n",
            "222\n",
            "78\n",
            "296\n",
            "216\n",
            "79\n",
            "338\n",
            "249\n",
            "80\n",
            "219\n",
            "169\n",
            "81\n",
            "240\n",
            "219\n",
            "82\n",
            "148\n",
            "335\n",
            "83\n",
            "177\n",
            "220\n",
            "84\n",
            "265\n",
            "205\n",
            "85\n",
            "261\n",
            "310\n",
            "86\n",
            "277\n",
            "223\n",
            "87\n",
            "247\n",
            "238\n",
            "88\n",
            "225\n",
            "251\n",
            "89\n",
            "302\n",
            "208\n",
            "90\n",
            "239\n",
            "197\n",
            "91\n",
            "196\n",
            "234\n",
            "92\n",
            "288\n",
            "285\n",
            "93\n",
            "224\n",
            "221\n",
            "94\n",
            "197\n",
            "221\n",
            "95\n",
            "224\n",
            "260\n",
            "96\n",
            "241\n",
            "232\n",
            "97\n",
            "253\n",
            "267\n",
            "98\n",
            "246\n",
            "246\n",
            "99\n",
            "304\n",
            "206\n",
            "-55.05659841010797\n",
            "[np.float64(176.22000714287435), np.float64(172.69131358640544)]\n",
            "wis\n",
            "-53.685850828271825\n",
            "[np.float64(137.31354574090406), np.float64(115.13089351613758)]\n",
            "dm\n",
            "-54.01073436235229\n",
            "[np.float64(132.81942955872594), np.float64(6.970382434455023)]\n",
            "dr\n",
            "-55.45433687243875\n",
            "[np.float64(168.14039815809465), np.float64(6.815543578139827)]\n",
            "dr3\n",
            "-55.35302467832383\n",
            "[np.float64(165.49802165849383), np.float64(5.193413671178172)]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-05-12T20:25:36.301160Z",
          "start_time": "2025-05-12T20:07:04.544166Z"
        },
        "id": "d1b9a15eca65f5db",
        "outputId": "ee940321-3c29-41d9-fbca-f5415733bb4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import numpy as np\n",
        "import sys\n",
        "import gym\n",
        "\n",
        "# Since lib.envs isn't available, we'll need to define these environments here\n",
        "# or use gym environments directly. For now, I'll create simplified versions.\n",
        "\n",
        "from collections import defaultdict\n",
        "\n",
        "# Simple CliffWalkingEnv implementation\n",
        "class CliffWalkingEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.shape = (4, 12)\n",
        "        self.start_state_index = np.ravel_multi_index((3, 0), self.shape)\n",
        "        self.goal_state_index = np.ravel_multi_index((3, 11), self.shape)\n",
        "        self.cliff = list(range(np.ravel_multi_index((3, 1), self.shape),\n",
        "                               np.ravel_multi_index((3, 11), self.shape)))\n",
        "        self.nS = self.shape[0] * self.shape[1]\n",
        "        self.nA = 4  # up, right, down, left\n",
        "\n",
        "        # Calculate transition probabilities and rewards\n",
        "        self.P = {}\n",
        "        for s in range(self.nS):\n",
        "            position = np.unravel_index(s, self.shape)\n",
        "            self.P[s] = {a: [] for a in range(self.nA)}\n",
        "\n",
        "            # Actions: 0=up, 1=right, 2=down, 3=left\n",
        "            for a in range(self.nA):\n",
        "                reward = -1.0  # default reward for each move\n",
        "                next_position = list(position)\n",
        "                if a == 0:\n",
        "                    next_position[0] = max(position[0] - 1, 0)\n",
        "                elif a == 1:\n",
        "                    next_position[1] = min(position[1] + 1, self.shape[1] - 1)\n",
        "                elif a == 2:\n",
        "                    next_position[0] = min(position[0] + 1, self.shape[0] - 1)\n",
        "                elif a == 3:\n",
        "                    next_position[1] = max(position[1] - 1, 0)\n",
        "\n",
        "                next_state = np.ravel_multi_index(next_position, self.shape)\n",
        "\n",
        "                # Check if we're at the cliff\n",
        "                if s in self.cliff:\n",
        "                    next_state = self.start_state_index\n",
        "                    reward = -100.0\n",
        "\n",
        "                # Check if we're at the goal\n",
        "                done = next_state == self.goal_state_index\n",
        "\n",
        "                self.P[s][a] = [(1.0, next_state, reward, done)]\n",
        "\n",
        "        self.observation_space = gym.spaces.Discrete(self.nS)\n",
        "        self.action_space = gym.spaces.Discrete(self.nA)\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        state, reward, done, _ = self._step(action)\n",
        "        self.s = state\n",
        "        return (state, reward, done, {})\n",
        "\n",
        "    def _step(self, action):\n",
        "        (probs, next_state, reward, done) = self.P[self.s][action][0]\n",
        "        return (next_state, reward, done, {})\n",
        "\n",
        "    def reset(self):\n",
        "        self.s = self.start_state_index\n",
        "        return self.s\n",
        "\n",
        "# Simple WindyGridworldEnv implementation (not fully used in this code)\n",
        "class WindyGridworldEnv(gym.Env):\n",
        "    def __init__(self):\n",
        "        self.shape = (7, 10)\n",
        "        self.nS = self.shape[0] * self.shape[1]\n",
        "        self.nA = 4  # up, right, down, left\n",
        "        self.wind = [0, 0, 0, 1, 1, 1, 2, 2, 1, 0]\n",
        "        self.reset()\n",
        "\n",
        "    def step(self, action):\n",
        "        # Not implemented as it's not used in the main code\n",
        "        pass\n",
        "\n",
        "    def reset(self):\n",
        "        self.s = np.ravel_multi_index((3, 0), self.shape)\n",
        "        return self.s\n",
        "\n",
        "from scipy.optimize import minimize, rosen, rosen_der\n",
        "from scipy.optimize import Bounds\n",
        "\n",
        "bounds = Bounds([-0.1, -0.1], [0.1, 0.1])\n",
        "\n",
        "env = CliffWalkingEnv()\n",
        "\n",
        "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
        "    def policy_fn(observation):\n",
        "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
        "        best_action = np.argmax(Q[observation])\n",
        "        A[best_action] += (1.0 - epsilon)\n",
        "        return A\n",
        "    return policy_fn\n",
        "\n",
        "Q_space = np.load(\"/content/drive/MyDrive/Work/Estimators/DoubleReinforcement/Q-table-cliff.npz\")[\"xxx\"]\n",
        "Q_space2 = np.load(\"/content/drive/MyDrive/Work/Estimators/DoubleReinforcement/Q-table-cliff.npz\")[\"xxx\"] #Q-table-cliff.npz\n",
        "\n",
        "prob1 = [1.0 for i in range((env.nA))]\n",
        "prob1 = prob1/np.sum(prob1)\n",
        "\n",
        "betabeta = 0.8\n",
        "def sample_policy(observation, alpha=0.9):\n",
        "    prob2 = alpha*Q_space[observation,:] + (1-alpha)*prob1\n",
        "    return np.random.choice(env.nA, 1, p=prob2)[0]\n",
        "\n",
        "\n",
        "def behavior_policy(observation, beta=betabeta):\n",
        "    prob2 = beta*Q_space[observation,:] + (1-beta)*prob1\n",
        "    return np.random.choice(env.nA, 1, p=prob2)[0]\n",
        "\n",
        "\n",
        "def target_dense(observation, alpha=0.9):\n",
        "    prob2 = alpha*Q_space[observation,:] + (1-alpha)*prob1\n",
        "    return prob2\n",
        "\n",
        "def behav_dense(observation, beta=betabeta):\n",
        "    prob2 = beta*Q_space[observation,:] + (1-beta)*prob1\n",
        "    return prob2\n",
        "\n",
        "def sarsa2(env, policy, policy2, num_episodes, discount_factor=1.0, Q_space2=Q_space2, alpha=0.6, epsilon=0.03):\n",
        "\n",
        "    Q = np.copy(Q_space2)\n",
        "    episode_episode = []\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "\n",
        "        if (i_episode + 1) % 200 == 0:\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        state = env.reset()\n",
        "        action = policy2(state)\n",
        "\n",
        "        episode = []\n",
        "\n",
        "        for t in itertools.count():\n",
        "            # Take a step\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            episode.append((state, action, reward))\n",
        "            # Pick the next action\n",
        "            next_action = policy2(next_state)\n",
        "\n",
        "            # TD Update\n",
        "            td_target = reward + discount_factor * np.sum(Q[next_state,:]*target_dense(next_state))\n",
        "            td_delta = td_target - Q[state, action]\n",
        "            Q[state, action] += alpha * td_delta\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            action = next_action\n",
        "            state = next_state\n",
        "\n",
        "        episode_episode.append(episode)\n",
        "\n",
        "    return Q, episode_episode\n",
        "\n",
        "bounds = Bounds([-0.2, -0.2], [0.2, 0.2])\n",
        "def sigmoid(x, derivative=False):\n",
        "    return x*(1-x) if derivative else 1/(1+np.exp(-x))\n",
        "\n",
        "\n",
        "depth = 1\n",
        "def mc_prediction(env, policy, policy2, episode_episode, Q_=1.0, num_episodes=100, discount_factor=1.0):\n",
        "\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "    returns_count2 = defaultdict(float)\n",
        "\n",
        "    predic_list = []\n",
        "    predic_list2 = []\n",
        "    predic_list3 = []\n",
        "    predic_list22 = []\n",
        "    predic_list4 = []\n",
        "    predic_list5 = np.ones(num_episodes)\n",
        "    auxiauxi = []\n",
        "    epiepi = []\n",
        "    weight_list = np.zeros([num_episodes, 1000])  # For bounded IPW\n",
        "    weight_list2 = np.zeros([num_episodes, 1002])  # For bounded IPW\n",
        "    weight_list3 = np.zeros([num_episodes, 1002])  # For bounded IPW\n",
        "    marginal_weight = np.zeros([num_episodes, 1000])  # For bounded IPW\n",
        "    marginal_weight_2 = np.zeros([num_episodes, 1000])  # For bounded IPW\n",
        "    auxi_list = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list2 = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list2_2 = np.zeros([num_episodes, 1000])\n",
        "    marginal_auxi_list_2 = np.zeros([num_episodes, 1000])\n",
        "    auxi_list2 = np.zeros([num_episodes, 1000])\n",
        "    reward_list = np.zeros([num_episodes, 1000])\n",
        "    state_list = np.zeros([num_episodes, 1000])\n",
        "    action_list = np.zeros([num_episodes, 1000])\n",
        "\n",
        "    count_list = np.zeros(1000)\n",
        "    episolode_longe_list = []\n",
        "\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "\n",
        "        if i_episode % 200 == 0:\n",
        "            sys.stdout.flush()\n",
        "\n",
        "        episode = episode_episode[i_episode]\n",
        "\n",
        "        W = 1.0\n",
        "        W_list = []\n",
        "        episolode_longe_list.append(len(episode))\n",
        "\n",
        "        weight_list2[i_episode, 0] = 1.0\n",
        "        for t in range(len(episode)):\n",
        "            state, action, reward = episode[t]\n",
        "            reward_list[i_episode, t] = reward\n",
        "            state_list[i_episode, t] = state\n",
        "            action_list[i_episode, t] = action\n",
        "\n",
        "            W = W*target_dense(state)[action]/behav_dense(state)[action]*discount_factor\n",
        "            probprob = 0.9*Q_space[state,:] + 0.1*prob1\n",
        "            W_list.append(W)\n",
        "            weight_list[i_episode, t] = W_list[t]\n",
        "            weight_list2[i_episode, t+1] = W_list[t]\n",
        "            weight_list3[i_episode, t] = target_dense(state)[action]/behav_dense(state)[action]\n",
        "\n",
        "            count_list[t] += 1.0\n",
        "\n",
        "            if t==0:\n",
        "                auxi_list[i_episode, t] = W_list[t]*Q_[state, action]-np.sum(probprob*Q_[state,:])\n",
        "            else:\n",
        "                auxi_list[i_episode, t] = W_list[t]*Q_[state, action]-W_list[t-1]*np.sum(probprob*Q_[state,:])\n",
        "\n",
        "            if t==0:\n",
        "                auxi_list2[i_episode, t] = W_list[t]-1.0\n",
        "            else:\n",
        "                auxi_list2[i_episode, t] = W_list[t]-W_list[t-1]\n",
        "\n",
        "    print(np.max(np.array(episolode_longe_list)))\n",
        "\n",
        "\n",
        "    weight_list_mean = np.mean(weight_list, 1)\n",
        "    reward_list_mean = np.mean(reward_list, 1)\n",
        "    auxi_list_mean = np.mean(auxi_list, 1)\n",
        "    auxi_list2_mean = np.mean(auxi_list2, 1)\n",
        "\n",
        "    val = []\n",
        "\n",
        "    ##### IPW\n",
        "    for i in range(num_episodes):\n",
        "        predic_list.append(np.sum(weight_list[i,:]*reward_list[i,:]))\n",
        "\n",
        "    val.append(np.mean(predic_list))\n",
        "\n",
        "    #### Marginalized-IPW\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        for j in range(episolode_longe_list[i]):\n",
        "            marginal_weight[i,j] = np.mean(weight_list[:,j][(state_list[:,j]==state_list[i,j]) & (action_list[:,j]==action_list[i,j])])\n",
        "            if j==0:\n",
        "                marginal_weight_2[i,j] = weight_list3[i,j]\n",
        "            else:\n",
        "                marginal_weight_2[i,j] = np.mean(weight_list[:,j-1][(state_list[:,j]==state_list[i,j])])*weight_list3[i,j]\n",
        "\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        for t in range(episolode_longe_list[i_episode]):\n",
        "            state = int(state_list[i_episode, t])  # Changed np.int to int\n",
        "            action = int(action_list[i_episode, t])  # Changed np.int to int\n",
        "            probprob = 0.9*Q_space[state,:] + 0.1*prob1\n",
        "            if t==0:\n",
        "                marginal_auxi_list[i_episode, t] = marginal_weight[i_episode, t]*Q_[state, action]-np.sum(probprob*Q_[state,:])\n",
        "                marginal_auxi_list_2[i_episode, t] = marginal_weight_2[i_episode, t]*Q_[state, action]-np.sum(probprob*Q_[state,:])\n",
        "                auxi_list[i_episode, t] = weight_list[i_episode, t]*Q_[state, action]-np.sum(probprob*Q_[state,:])\n",
        "            else:\n",
        "                marginal_auxi_list[i_episode, t] = marginal_weight[i_episode, t]*(Q_[state, action])-marginal_weight[i_episode, t-1]*np.sum(probprob*(Q_[state,:]))\n",
        "                marginal_auxi_list_2[i_episode, t] = marginal_weight_2[i_episode, t]*(Q_[state, action])-marginal_weight_2[i_episode, t-1]*np.sum(probprob*(Q_[state,:]))\n",
        "                auxi_list[i_episode, t] = weight_list[i_episode, t]*(Q_[state, action])-weight_list[i_episode, t-1]*np.sum(probprob*(Q_[state,:]))\n",
        "\n",
        "            if t==0:\n",
        "                marginal_auxi_list2[i_episode, t] = marginal_weight[i_episode, t]-1.0\n",
        "                marginal_auxi_list2_2[i_episode, t] = marginal_weight_2[i_episode, t]-1.0\n",
        "                auxi_list2[i_episode, t] = weight_list[i_episode, t]-1.0\n",
        "            else:\n",
        "                marginal_auxi_list2[i_episode, t] = marginal_weight[i_episode, t]- marginal_weight[i_episode, t-1]\n",
        "                marginal_auxi_list2_2[i_episode, t] = marginal_weight_2[i_episode, t]- marginal_weight_2[i_episode, t-1]\n",
        "                auxi_list2[i_episode, t] = weight_list[i_episode, t]-weight_list[i_episode, t-1]\n",
        "\n",
        "\n",
        "    for i in range(num_episodes):\n",
        "        predic_list2.append(np.sum(marginal_weight[i,:]*reward_list[i,:]))\n",
        "\n",
        "    ### marginal ipw2  #### Using action and state\n",
        "    val.append(np.mean(predic_list2))\n",
        "\n",
        "\n",
        "    ### marginal ipw3#### Using only state\n",
        "    for i in range(num_episodes):\n",
        "        predic_list22.append(np.sum(marginal_weight_2[i,:]*reward_list[i,:]))\n",
        "\n",
        "    val.append(np.mean(predic_list22))\n",
        "\n",
        "\n",
        "    #### DR\n",
        "    val.append(np.mean(predic_list)-np.mean(np.sum(auxi_list, 1)))\n",
        "\n",
        "    #### marginal DR 1  #### Using action and state\n",
        "    val.append(np.mean(predic_list2)-np.mean(np.sum(marginal_auxi_list, 1)))\n",
        "    #### marginal DR 2   #### Using only state\n",
        "    val.append(np.mean(predic_list22)-np.mean(np.sum(marginal_auxi_list_2, 1)))\n",
        "\n",
        "    return val\n",
        "\n",
        "# Main experiment run\n",
        "is_list = []\n",
        "is2_list = []\n",
        "is3_list = []\n",
        "wis_list = []\n",
        "wis2_list = []\n",
        "dm_list = []\n",
        "dr_list = []\n",
        "dr2_list = []\n",
        "dr3_list = []\n",
        "bdr_list = []\n",
        "drs_list = []\n",
        "drs2_list = []\n",
        "drss_list = []\n",
        "mdr_list = []\n",
        "mdr_list2 = []\n",
        "\n",
        "sample_size = 1000\n",
        "sample_size = sample_size // 2  # Integer division in Python 3\n",
        "for kkk in range(100):\n",
        "    print(kkk)\n",
        "    #### Sample splititng\n",
        "    ### First fold\n",
        "\n",
        "    predicted_Q, episode_episode = sarsa2(env, sample_policy, behavior_policy, sample_size)\n",
        "    V_10k_1 = mc_prediction(env, sample_policy, behavior_policy, episode_episode, predicted_Q, num_episodes=sample_size)\n",
        "\n",
        "    ### Second fold\n",
        "    predicted_Q, episode_episode = sarsa2(env, sample_policy, behavior_policy, sample_size)\n",
        "    V_10k_2 = mc_prediction(env, sample_policy, behavior_policy, episode_episode, predicted_Q, num_episodes=sample_size)\n",
        "\n",
        "    V_10k = 0.5*(np.array(V_10k_1)+np.array(V_10k_2))\n",
        "    is_list.append(np.mean(V_10k[0]))\n",
        "    is2_list.append(np.mean(V_10k[1]))\n",
        "    is3_list.append(np.mean(V_10k[2]))\n",
        "    dr_list.append(np.mean(V_10k[3]))\n",
        "    dr2_list.append(np.mean(V_10k[4]))\n",
        "    dr3_list.append(np.mean(V_10k[5]))\n",
        "    probprob = 0.9*Q_space[36,:] + 0.1*prob1\n",
        "    dm_list.append(np.sum(probprob*predicted_Q[36,:]))\n",
        "    np.savez(\"2estimator_list_ipw_\"+str(betabeta)+\"_\"+str(sample_size), a=is_list)\n",
        "    np.savez(\"2estimator_list_ipw2_\"+str(betabeta)+\"_\"+str(sample_size), a=is3_list)\n",
        "    np.savez(\"2estimator_list_dm_\"+str(betabeta)+\"_\"+str(sample_size), a=dm_list)\n",
        "    np.savez(\"2estimator_list_dr_\"+str(betabeta)+\"_\"+str(sample_size), a=dr_list)\n",
        "    np.savez(\"2estimator_list_dr2_\"+str(betabeta)+\"_\"+str(sample_size), a=dr3_list)\n",
        "\n",
        "# Analysis of results\n",
        "true = -42.49\n",
        "\n",
        "# FIX: Properly calculate MSE instead of using hardcoded values\n",
        "def mse(aaa):\n",
        "    aaa = np.array(aaa)\n",
        "    aaa = aaa[aaa>-100]  # Filter extreme values\n",
        "    mean_val = np.mean(aaa)  # Calculate mean\n",
        "    bias = mean_val - true  # Calculate bias\n",
        "    bias_squared = bias * bias  # Square the bias\n",
        "    variance = np.var(aaa)  # Calculate variance\n",
        "    mse_value = bias_squared + variance  # MSE = bias² + variance\n",
        "    return [mse_value, np.sqrt(np.var((aaa-true)*(aaa-true)))]  # Return MSE and RMSE\n",
        "\n",
        "print(np.mean(is_list))\n",
        "print(mse(is_list))\n",
        "print(\"wis\")\n",
        "print(np.mean(is3_list))\n",
        "print(mse(is3_list))\n",
        "print(\"dm\")\n",
        "print(np.mean(dm_list))\n",
        "print(mse(dm_list))\n",
        "print(\"dr\")\n",
        "print(np.mean(dr_list))\n",
        "print(mse(dr_list))\n",
        "print(\"dr3\")\n",
        "print(np.mean(dr3_list))\n",
        "print(mse(dr3_list))"
      ],
      "id": "d1b9a15eca65f5db",
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Work/Estimators/DoubleReinforcement/Q-table-cliff.npz'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-1fc03caddcee>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0mQ_space\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Work/Estimators/DoubleReinforcement/Q-table-cliff.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"xxx\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0mQ_space2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/Work/Estimators/DoubleReinforcement/Q-table-cliff.npz\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"xxx\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m#Q-table-cliff.npz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/lib/_npyio_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Work/Estimators/DoubleReinforcement/Q-table-cliff.npz'"
          ]
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ij5Uepr11-NZ"
      },
      "id": "ij5Uepr11-NZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}